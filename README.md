This notebook is on text summarisation using LLM models. We explore prompt engineering by comparing zero shot, one shot and few shot inferences and then try out different generative configuration parameters for inference.

Without fine tuning these are the two things we can do to affect the output of a specific LLM model: Instruction fine tuning (prompt engineering) and configuring the parameters for inference.
